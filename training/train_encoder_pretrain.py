"""Encoder pre-training — ChessEncoder MSE regression on cp_diff_scaled.

Trains ONLY the CNN trunk + a scalar regression head.
No LLM involved — this is a cheap warm-start before the full SFT integration.

Input data (generated by data/pipeline/generate_encoder_data.py):
    {"fen": "...", "move_uci": "e2e4", "cp_diff_scaled": 0.12}

Architecture:
    ChessEncoder(38-ch) → Linear(out_dim → 1) → tanh → cp_diff_hat ∈ (-1, 1)
    Loss: MSE(cp_diff_hat, cp_diff_scaled)

After pre-training, the CNN weights can be reused in ChessLMWithEncoder by
loading only the cnn.* keys from the checkpoint state_dict.

Usage (single GPU):
    uv run python training/train_encoder_pretrain.py \
        --config training/configs/encoder_pretrain.yaml

Usage (2-GPU DDP):
    torchrun --nproc_per_node=2 training/train_encoder_pretrain.py \
        --config training/configs/encoder_pretrain.yaml
"""

from __future__ import annotations

import argparse
import json
import logging
import math
import os
import sys
import time
from pathlib import Path

import chess
import torch
import torch.distributed as dist
import torch.nn as nn
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, Dataset, DistributedSampler

sys.path.insert(0, str(Path(__file__).parent.parent))
from src.encoder.board_tensor import boards_to_tensor
from src.encoder.cnn import ChessEncoder
from training.lib import load_config

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
)
logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Dataset
# ---------------------------------------------------------------------------


class EncoderPretrainDataset(Dataset):
    """Dataset of (board_tensor, cp_diff_scaled) pairs from fishnet-evals."""

    def __init__(self, jsonl_path: str, limit: int = 0) -> None:
        self.records: list[dict] = []
        with open(jsonl_path, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    self.records.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
                if limit and len(self.records) >= limit:
                    break
        logger.info("Loaded %d records from %s", len(self.records), jsonl_path)

    def __len__(self) -> int:
        return len(self.records)

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:
        rec = self.records[idx]
        board = chess.Board(rec["fen"])
        try:
            move = chess.Move.from_uci(rec["move_uci"])
        except Exception:
            move = None
        board_tensor = boards_to_tensor(board, move)  # (38, 8, 8)
        label = torch.tensor(rec["cp_diff_scaled"], dtype=torch.float32)
        return board_tensor, label


# ---------------------------------------------------------------------------
# Model: CNN + scalar head
# ---------------------------------------------------------------------------


class EncoderRegressorHead(nn.Module):
    """Thin scalar head on top of ChessEncoder for cp_diff regression.

    Adds a single Linear(out_dim → 1) + tanh to keep output in (-1, 1).
    The tanh matches the label range from _cp_diff_scaled.
    """

    def __init__(self, encoder: ChessEncoder) -> None:
        super().__init__()
        self.encoder = encoder
        self.head = nn.Linear(encoder.proj.out_features, 1)

    def forward(self, board_tensor: torch.Tensor) -> torch.Tensor:
        """Forward pass.

        Args:
            board_tensor: (B, 38, 8, 8)

        Returns:
            (B,) predictions in (-1, 1)
        """
        features = self.encoder(board_tensor)  # (B, out_dim)
        logit = self.head(features).squeeze(-1)  # (B,)
        return torch.tanh(logit)


# ---------------------------------------------------------------------------
# Training loop
# ---------------------------------------------------------------------------


def setup_ddp() -> tuple[int, int, int]:
    """Initialize DDP if launched with torchrun, else single-GPU."""
    if "RANK" in os.environ:
        dist.init_process_group(backend="nccl")
        rank = dist.get_rank()
        local_rank = int(os.environ["LOCAL_RANK"])
        world_size = dist.get_world_size()
    else:
        rank = 0
        local_rank = 0
        world_size = 1
    return rank, local_rank, world_size


def cleanup_ddp() -> None:
    if dist.is_initialized():
        dist.destroy_process_group()


def main() -> None:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--config",
        "-c",
        default="recipes-train/encoder-pretrain/config.yaml",
        help="Path to YAML config",
    )
    parser.add_argument("--resume", default=None, help="Path to checkpoint to resume from")
    args = parser.parse_args()

    rank, local_rank, world_size = setup_ddp()
    device = torch.device(f"cuda:{local_rank}" if torch.cuda.is_available() else "cpu")
    is_main = rank == 0

    config = load_config(args.config)
    train_cfg = config.get("training", {})
    encoder_cfg = config.get("encoder", {})
    wandb_cfg = config.get("wandb", {})
    output_dir = Path(config.get("output_dir", "checkpoints/encoder-pretrain"))

    if is_main:
        output_dir.mkdir(parents=True, exist_ok=True)
        if wandb_cfg.get("enabled"):
            import wandb

            wandb.init(
                project=wandb_cfg.get("project", "chess-tutor"),
                name=wandb_cfg.get("name", "encoder-pretrain"),
                tags=wandb_cfg.get("tags", ["encoder-pretrain"]),
                config=config,
            )

    # ── Dataset ──────────────────────────────────────────────────────────────
    train_ds = EncoderPretrainDataset(
        train_cfg["train_file"],
        limit=train_cfg.get("train_limit", 0),
    )
    eval_ds = None
    if train_cfg.get("eval_file"):
        eval_ds = EncoderPretrainDataset(
            train_cfg["eval_file"],
            limit=train_cfg.get("eval_limit", 0),
        )

    train_sampler = DistributedSampler(train_ds, shuffle=True) if world_size > 1 else None
    train_loader = DataLoader(
        train_ds,
        batch_size=train_cfg.get("per_device_train_batch_size", 256),
        sampler=train_sampler,
        shuffle=(train_sampler is None),
        num_workers=train_cfg.get("dataloader_num_workers", 4),
        pin_memory=True,
    )

    eval_sampler = DistributedSampler(eval_ds, shuffle=False) if world_size > 1 else None
    eval_loader = None
    if eval_ds is not None:
        eval_loader = DataLoader(
            eval_ds,
            batch_size=train_cfg.get("per_device_eval_batch_size", 512),
            sampler=eval_sampler,
            shuffle=False,
            num_workers=train_cfg.get("dataloader_num_workers", 4),
            pin_memory=True,
        )

    # ── Model ─────────────────────────────────────────────────────────────────
    encoder = ChessEncoder(
        in_channels=encoder_cfg.get("in_channels", 38),
        hidden_size=encoder_cfg.get("hidden_size", 256),
        num_blocks=encoder_cfg.get("num_blocks", 10),
        out_dim=encoder_cfg.get("out_dim", 2560),
    )
    model = EncoderRegressorHead(encoder).to(device)

    if args.resume:
        ckpt = torch.load(args.resume, map_location=device, weights_only=True)
        model.load_state_dict(ckpt["model"])
        if is_main:
            logger.info("Resumed from %s (epoch %d)", args.resume, ckpt.get("epoch", -1))

    if world_size > 1:
        model = DDP(model, device_ids=[local_rank])

    # ── Optimizer ─────────────────────────────────────────────────────────────
    lr = train_cfg.get("learning_rate", 3e-4)
    weight_decay = train_cfg.get("weight_decay", 0.01)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

    num_epochs = train_cfg.get("num_train_epochs", 3)
    steps_per_epoch = math.ceil(
        len(train_ds) / (train_cfg.get("per_device_train_batch_size", 256) * world_size)
    )
    total_steps = num_epochs * steps_per_epoch
    warmup_steps = int(total_steps * train_cfg.get("warmup_ratio", 0.03))

    def lr_lambda(step: int) -> float:
        if step < warmup_steps:
            return step / max(1, warmup_steps)
        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)
        return 0.5 * (1.0 + math.cos(math.pi * progress))

    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    criterion = nn.MSELoss()

    logging_steps = train_cfg.get("logging_steps", 100)
    save_steps = train_cfg.get("save_steps", 1000)
    eval_steps = train_cfg.get("eval_steps", 1000)
    save_total_limit = train_cfg.get("save_total_limit", 5)
    max_grad_norm = train_cfg.get("max_grad_norm", 1.0)
    bf16 = train_cfg.get("bf16", True)

    scaler = torch.amp.GradScaler("cuda") if not bf16 else None
    amp_dtype = torch.bfloat16 if bf16 else torch.float16

    global_step = 0
    best_eval_loss = float("inf")
    saved_checkpoints: list[Path] = []

    def save_checkpoint(step: int, epoch: int, eval_loss: float | None = None) -> None:
        nonlocal saved_checkpoints
        ckpt_dir = output_dir / f"checkpoint-{step}"
        ckpt_dir.mkdir(parents=True, exist_ok=True)
        raw_model = model.module if isinstance(model, DDP) else model
        torch.save(
            {
                "model": raw_model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "scheduler": scheduler.state_dict(),
                "step": step,
                "epoch": epoch,
                "eval_loss": eval_loss,
            },
            ckpt_dir / "checkpoint.pt",
        )
        # Also save just the encoder weights for easy loading into ChessLMWithEncoder
        encoder_state = {
            k.removeprefix("encoder."): v
            for k, v in raw_model.state_dict().items()
            if k.startswith("encoder.")
        }
        torch.save(encoder_state, ckpt_dir / "encoder_weights.pt")
        logger.info("Saved checkpoint to %s", ckpt_dir)

        saved_checkpoints.append(ckpt_dir)
        if save_total_limit and len(saved_checkpoints) > save_total_limit:
            oldest = saved_checkpoints.pop(0)
            import shutil

            shutil.rmtree(oldest, ignore_errors=True)
            logger.info("Removed old checkpoint %s", oldest)

    def run_eval() -> float:
        if eval_loader is None:
            return float("nan")
        raw_model = model.module if isinstance(model, DDP) else model
        raw_model.eval()
        total_loss = 0.0
        n = 0
        with torch.no_grad():
            for boards, labels in eval_loader:
                boards = boards.to(device, non_blocking=True)
                labels = labels.to(device, non_blocking=True)
                with torch.amp.autocast("cuda", dtype=amp_dtype):
                    preds = raw_model(boards)
                    loss = criterion(preds, labels)
                total_loss += loss.item() * boards.size(0)
                n += boards.size(0)
        raw_model.train()
        t = torch.tensor([total_loss, float(n)], dtype=torch.float64, device=device)
        if world_size > 1:
            dist.all_reduce(t, op=dist.ReduceOp.SUM)
        avg = (t[0] / t[1]).item() if t[1] > 0 else 0.0
        return avg

    # ── Training ───────────────────────────────────────────────────────────────
    if is_main:
        param_count = sum(p.numel() for p in model.parameters())
        logger.info(
            "Starting encoder pre-training | params=%s | steps_per_epoch=%d | total_steps=%d",
            f"{param_count:,}",
            steps_per_epoch,
            total_steps,
        )

    model.train()
    t0 = time.time()

    for epoch in range(num_epochs):
        if train_sampler is not None:
            train_sampler.set_epoch(epoch)

        running_loss = 0.0
        running_n = 0

        for boards, labels in train_loader:
            boards = boards.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            optimizer.zero_grad()
            with torch.amp.autocast("cuda", dtype=amp_dtype):
                preds = model(boards)
                loss = criterion(preds, labels)

            if scaler is not None:
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                optimizer.step()

            scheduler.step()
            global_step += 1
            running_loss += loss.item() * boards.size(0)
            running_n += boards.size(0)

            if is_main and global_step % logging_steps == 0:
                avg_loss = running_loss / running_n
                elapsed = time.time() - t0
                lr_now = scheduler.get_last_lr()[0]
                logger.info(
                    "step=%d  epoch=%d  loss=%.4f  lr=%.2e  elapsed=%.1fs",
                    global_step,
                    epoch + 1,
                    avg_loss,
                    lr_now,
                    elapsed,
                )
                if wandb_cfg.get("enabled"):
                    import wandb

                    wandb.log({"train/loss": avg_loss, "train/lr": lr_now}, step=global_step)
                running_loss = 0.0
                running_n = 0

            if global_step % eval_steps == 0:
                eval_loss = run_eval()
                if is_main:
                    logger.info("step=%d  eval_loss=%.4f", global_step, eval_loss)
                    if wandb_cfg.get("enabled"):
                        import wandb

                        wandb.log({"eval/loss": eval_loss}, step=global_step)

            if global_step % save_steps == 0:
                if eval_steps != save_steps:
                    eval_loss_for_ckpt = run_eval()
                elif global_step % eval_steps == 0:
                    eval_loss_for_ckpt = eval_loss
                else:
                    eval_loss_for_ckpt = float("nan")
                if is_main:
                    save_checkpoint(global_step, epoch + 1, eval_loss_for_ckpt)

    # Final save
    if is_main:
        final_eval = run_eval()
        logger.info("Training done. Final eval loss: %.4f", final_eval)
        save_checkpoint(global_step, num_epochs, final_eval)

    cleanup_ddp()


if __name__ == "__main__":
    main()
