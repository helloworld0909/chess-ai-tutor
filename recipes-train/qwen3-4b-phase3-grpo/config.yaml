# GRPO configuration — Qwen3-4B-Thinking-2507 + QLoRA (8-bit, r=64)
# Stage 1: Line generator RL with verifiable chess rewards
#
# Parallelism: 2-GPU DDP via torchrun (each rank = full model replica on own GPU)
# Rewards: R1 legality (gate) + R2 eval accuracy (0.28) + R3a annotation (0.12)
#          + R4 depth (0.10) + R5 breadth (0.10) + R6 relevance (0.05)
# R1<0 short-circuits all downstream rewards to -1.0

model:
  model_name: "checkpoints/qwen3-4b-phase2-lines-sft/checkpoint-350"
  quantization: "8bit"
  attn_implementation: "sdpa"
  torch_dtype: bfloat16

lora:
  r: 64
  alpha: 128
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  bias: none

training:
  train_file: data/processed/lines_sft.jsonl
  eval_file:  data/processed/lines_sft_eval.jsonl

  # Rollout generation
  # 2-GPU DDP via torchrun — each rank gets a full model replica on its own GPU.
  # OOM fix: reduce per_device_train_batch_size 4→2 and num_generations 4→2 to cut
  # backward activation footprint from 16 rollouts/GPU to 4 rollouts/GPU.
  # Constraint: per_device_train_batch_size must be divisible by num_generations.
  num_generations: 2          # reduced from 4; 2 completions per prompt per GPU
  max_prompt_length: 800      # tokens (system + user — measured p99=641, max=644; 800 = safe ceiling)
  max_completion_length: 1500 # thinking (~1000) + 3-5 lines (~500) — never truncate mid-output
  stockfish_depth: 6          # depth 6 ≈ 16× faster than 12, ~2200 Elo — sufficient for reward signal

  # Batch — was 4 prompts × 4 gen = 16 rollouts/GPU → OOM during backward at step 6
  # Now 2 prompts × 2 gen = 4 rollouts/GPU; effective batch same via more accumulation:
  # 2 prompts/GPU × 2 GPUs × 2 gen × 8 accum = 64 rollouts
  per_device_train_batch_size: 2   # per GPU; divisible by num_generations=2 ✓
  gradient_accumulation_steps: 8   # effective batch = 2 × 2 × 2 × 8 = 64 rollouts

  # Optimiser
  learning_rate: 5.0e-6
  lr_scheduler_type: cosine
  warmup_ratio: 0.05
  num_train_epochs: 1
  optim: adamw_8bit
  weight_decay: 0.01
  max_grad_norm: 0.1

  # GRPO hyperparameters
  beta: 0.04      # KL penalty — keeps policy close to SFT checkpoint
  epsilon: 0.2    # PPO clip ratio

  # Generation sampling
  temperature: 0.9
  top_p: 0.95

  # Infrastructure
  bf16: true
  gradient_checkpointing: true
  seed: 42

  # Logging / checkpointing
  logging_steps: 5
  eval_strategy: steps
  eval_steps: 100
  save_strategy: steps
  save_steps: 100
  save_total_limit: 5

output_dir: checkpoints/qwen3-4b-phase3-grpo

wandb:
  enabled: false
  project: chess-tutor-grpo
  name: qwen3-4b-grpo-phase1
  tags:
    - grpo
    - lines
    - phase1
