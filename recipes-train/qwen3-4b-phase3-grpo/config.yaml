# GRPO configuration — Qwen3-4B-Thinking-2507 + QLoRA (8-bit, r=64)
# Stage 1: Line generator RL with verifiable chess rewards
#
# Parallelism: 2-GPU DDP via torchrun (each rank = full model replica on own GPU)
# Rewards: R1 legality (gate) + R2 eval accuracy (0.28) + R3a annotation (0.12)
#          + R4 depth (0.10) + R5 breadth (0.10) + R6 relevance (0.05)
# R1<0 short-circuits all downstream rewards to -1.0

model:
  model_name: "checkpoints/qwen3-4b-phase2-lines-sft"
  quantization: "8bit"
  attn_implementation: "sdpa"
  torch_dtype: bfloat16

lora:
  r: 64
  alpha: 128
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  bias: none

training:
  train_file: data/processed/lines_sft_thinking.jsonl
  eval_file:  data/processed/lines_sft_thinking_eval.jsonl

  # Rollout generation
  # 2-GPU DDP via torchrun — each rank gets a full model replica on its own GPU.
  # Constraint: per_device_train_batch_size must be divisible by num_generations.
  num_generations: 4          # completions per prompt — keep 4 for GRPO advantage variance
  max_prompt_length: 800      # tokens (system + user — measured p99=641, max=644; 800 = safe ceiling)
  max_completion_length: 1500 # keep high — want to encourage deep thinking
  stockfish_depth: 12         # CPU is idle, use full depth for better reward signal

  # Batch — 11-13GB VRAM free → bump per_device 2→3 safely (~25GB peak)
  # TRL constraint: generation_batch_size = per_device × world_size × grad_accum must be
  # divisible by num_generations=4 → 3 × 2 × 4 = 24, 24 % 4 == 0 ✓
  # grad_accum 8→4 halves backward passes per optimizer step (speed), keeps batch reasonable
  # Effective batch: 3 prompts/GPU × 2 GPUs × 4 gen × 4 accum = 96 rollouts
  per_device_train_batch_size: 3
  gradient_accumulation_steps: 4

  # Optimiser
  learning_rate: 5.0e-6
  lr_scheduler_type: cosine
  warmup_steps: 10         # short warmup — LoRA init is small, no need for 236-step ramp
  num_train_epochs: 1
  optim: adamw_8bit
  weight_decay: 0.01
  max_grad_norm: 0.1

  # GRPO hyperparameters
  # beta=0: TRL skips the reference model entirely (ref_model=None), saving ~5GB VRAM
  # per GPU and one full forward pass per step. The Phase 2 SFT reference is weak
  # (produces illegal moves), so there is no benefit to KL regularization here.
  beta: 0.0       # no KL penalty — ref model skipped entirely, saves VRAM + compute
  epsilon: 0.2    # PPO clip ratio

  # Generation sampling
  temperature: 0.9
  top_p: 0.95

  # Infrastructure
  bf16: true
  gradient_checkpointing: true
  seed: 42

  # Logging / checkpointing
  logging_steps: 5
  eval_strategy: steps
  eval_steps: 100
  save_strategy: steps
  save_steps: 10
  save_total_limit: 20

output_dir: checkpoints/qwen3-4b-phase3-grpo

wandb:
  enabled: false
  project: chess-tutor-grpo
  name: qwen3-4b-grpo-phase1
  tags:
    - grpo
    - lines
    - phase1
