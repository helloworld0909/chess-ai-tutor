# Encoder CNN pre-training — MSE regression on cp_diff_scaled
# Trains ONLY ChessEncoder + scalar head (no LLM).
# Input: data/processed/encoder_pretrain.jsonl (from generate_encoder_data.py)
# Output: checkpoints/encoder-pretrain/checkpoint-*/encoder_weights.pt
#         → load these CNN weights into ChessLMWithEncoder for the full SFT run.
#
# DDP: torchrun --nproc_per_node=2 training/train_encoder_pretrain.py
# Single GPU: uv run python training/train_encoder_pretrain.py

encoder:
  in_channels: 38       # before (19ch) + after (19ch) board tensors
  hidden_size: 512      # ResNet filters
  num_blocks: 15        # depth
  out_dim: 2560         # must match Qwen3-4B hidden_size for later SFT integration

training:
  train_file: data/processed/encoder_pretrain.jsonl
  eval_file:  data/processed/encoder_pretrain_eval.jsonl  # optional; omit if not available
  train_limit: 0        # 0 = no limit (use full file)
  eval_limit: 1000000   # use full 1M eval data

  per_device_train_batch_size: 256
  per_device_eval_batch_size: 2048
  dataloader_num_workers: 2
  
  learning_rate: 3.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.03
  num_train_epochs: 3
  max_grad_norm: 1.0

  logging_steps: 200
  eval_steps: 10000
  save_steps: 10000
  save_total_limit: 5

  bf16: true            # bfloat16 AMP (no GradScaler needed)

output_dir: checkpoints/encoder-pretrain

wandb:
  enabled: false
  project: chess-tutor
  name: encoder-pretrain
  tags:
    - encoder-pretrain
